<header>        <img src="file:///D:/Pictures/å›¾ç‰‡1.png" alt="å›¾ç‰‡æè¿°">        <p>äººå·¥æ™ºèƒ½å®éªŒ</p>    </header>

# ä¸­å±±å¤§å­¦è®¡ç®—æœºå­¦é™¢

# äººå·¥æ™ºèƒ½

# æœ¬ç§‘ç”Ÿå®éªŒæŠ¥å‘Š

###### ï¼ˆ2023å­¦å¹´æ˜¥å­£å­¦æœŸï¼‰



**è¯¾ç¨‹åç§°**ï¼šArtificial Intelligence



## ä¸€ã€å®éªŒé¢˜ç›®

**æ ‡é¢˜ï¼š** ä½¿ç”¨ Deep Q-learning Network(DQN) ç®—æ³•ç©è§£å†³ CartPole-v1 æ¸¸æˆ

**å†…å®¹æè¿°ï¼š** æœ¬å®éªŒæ—¨åœ¨ä½¿ç”¨æ·±åº¦ Q å­¦ä¹ ç½‘ç»œï¼ˆDQNï¼‰ç®—æ³•æ¥è§£å†³ OpenAI Gym ä¸­çš„ CartPole-v1 æ¸¸æˆã€‚æˆ‘ä»¬æä¾›äº†æ¡†æ¶ä»£ç çš„åŸºç¡€éƒ¨åˆ†ï¼Œä½†ä»éœ€è¦è¡¥å……éƒ¨åˆ†ä»£ç ä»¥å®Œæˆæ•´ä¸ªå®éªŒã€‚

**åŸºç¡€è¦æ±‚å®Œæˆå¦‚ä¸‹ä»»åŠ¡ï¼š**

- åœ¨ 500 å±€æ¸¸æˆå†…ï¼Œè‡³å°‘è¾¾æˆä¸€æ¬¡ï¼šè¿ç»­ 10 å±€ Reward å€¼ä¸º 500 çš„æƒ…å†µã€‚
- å±•ç¤ºå•å±€ Reward å€¼æ›²çº¿ä»¥åŠæœ€è¿‘ 100 å±€çš„å¹³å‡ Reward å€¼æ›²çº¿ã€‚

**è¿›é˜¶ç›®æ ‡åŒ…æ‹¬ä½†ä¸é™äºï¼š**

- è¾¾æˆä¸€æ¬¡ï¼šâ€œæœ€è¿‘ç™¾å±€çš„å¹³å‡ Reward å€¼â€â‰¥ 475ã€‚
- æ›´å¿«åœ°è¾¾åˆ°è¿™ä¸ªç›®æ ‡ã€‚
- å®ç°æ›´é«˜çš„ç™¾å±€å¹³å‡ Reward å€¼ã€‚

**éœ€è¦è¡¥å……çš„ä»£ç åŒ…æ‹¬ä»¥ä¸‹éƒ¨åˆ†ï¼š**

1. **Q ç½‘ç»œï¼ˆQnetï¼‰ï¼š**
   - è¡¥å……ä¸€ä¸ªçº¿æ€§å±‚ä»¥å®Œå–„ Q ç½‘ç»œçš„ç»“æ„ã€‚
2. **ç»éªŒå›æ”¾ç¼“å†²åŒºï¼ˆReplayBufferï¼‰ï¼š**
   - å®ç°ç»éªŒå›æ”¾ç¼“å†²åŒºçš„æ‰€æœ‰æˆå‘˜å‡½æ•°ï¼ŒåŒ…æ‹¬æ·»åŠ ç»éªŒã€é‡‡æ ·ç­‰ã€‚
3. **DQN ç®—æ³•çš„å…·ä½“å®ç°ï¼ˆDQNï¼‰ï¼š**
   - `choose_action` å‡½æ•°ï¼šå®ç° ğœ–-greedy ç­–ç•¥çš„ä»£ç ï¼Œç”¨äºé€‰æ‹©åŠ¨ä½œã€‚
   - `learn` å‡½æ•°ï¼š
     - å®ç° Q å€¼çš„è®¡ç®—ã€‚
     - å®ç°ç›®æ ‡å€¼çš„è®¡ç®—ã€‚
     - å®ç°æŸå¤±å€¼çš„è®¡ç®—ã€‚
     - å®ç°æ¢¯åº¦ä¸‹é™è¿‡ç¨‹ã€‚

ä»¥ä¸Šæ˜¯åŸºæœ¬è¦æ±‚ï¼Œæ ¹æ®å®éªŒéœ€æ±‚ï¼Œå¯ä»¥è¿›ä¸€æ­¥è°ƒæ•´å’Œæ”¹è¿›ç®—æ³•ï¼Œä»¥è·å¾—æ›´å¥½çš„ç»“æœã€‚

**ç»“æœè¦æ±‚ä¸å±•ç¤ºï¼š**

1. å•å±€ Reward å€¼æ›²çº¿ï¼šå±•ç¤ºæ¯å±€æ¸¸æˆçš„ Reward å€¼éšæ—¶é—´çš„å˜åŒ–æƒ…å†µã€‚
2. æœ€è¿‘ 100 å±€çš„å¹³å‡ Reward å€¼æ›²çº¿ï¼šå±•ç¤ºæœ€è¿‘ 100 å±€æ¸¸æˆçš„å¹³å‡ Reward å€¼éšæ—¶é—´çš„å˜åŒ–æƒ…å†µã€‚

## äºŒã€å®éªŒå†…å®¹

### 1.ç®—æ³•åŸç†

* Deep Q-learning Nextword(DQN)

  æ·±åº¦ Q å­¦ä¹ ç½‘ç»œï¼ˆDQNï¼‰æ˜¯å°†æ·±åº¦å­¦ä¹ æŠ€æœ¯ä¸ Q-learning ç›¸ç»“åˆçš„ä¸€ç§ç®—æ³•ï¼Œæ—¨åœ¨å¤„ç†çŠ¶æ€ç©ºé—´è¾ƒå¤§æˆ–è¿ç»­çš„é—®é¢˜ã€‚å…¶åŸºæœ¬åŸç†å¦‚ä¸‹ï¼š

  - **çŠ¶æ€è¡¨ç¤ºï¼š** çŠ¶æ€è¢«è¡¨ç¤ºä¸ºè¾“å…¥ç¥ç»ç½‘ç»œçš„ç‰¹å¾ã€‚
  - **åŠ¨ä½œé€‰æ‹©ç­–ç•¥ï¼š** ä½¿ç”¨ ğœ–-greedy ç­–ç•¥ï¼Œä»¥ä¸€å®šçš„æ¦‚ç‡éšæœºé€‰æ‹©åŠ¨ä½œï¼Œä»¥æ¢ç´¢ç¯å¢ƒï¼Œè€Œä»¥è¾ƒå¤§çš„æ¦‚ç‡é€‰æ‹©å½“å‰ Q å€¼æœ€å¤§çš„åŠ¨ä½œï¼Œä»¥åˆ©ç”¨å·²æœ‰çŸ¥è¯†ã€‚
  - **Q ç½‘ç»œç»“æ„ï¼š** ä½¿ç”¨ç¥ç»ç½‘ç»œæ¥è¿‘ä¼¼ Q å€¼å‡½æ•°ã€‚è¯¥ç½‘ç»œçš„è¾“å…¥ä¸ºçŠ¶æ€ï¼Œè¾“å‡ºä¸ºæ¯ä¸ªå¯èƒ½åŠ¨ä½œçš„ Q å€¼ã€‚
  - **ç»éªŒå›æ”¾ï¼š** å°†æ™ºèƒ½ä½“çš„ç»éªŒå­˜å‚¨åœ¨å›æ”¾ç¼“å†²åŒºä¸­ï¼Œç„¶åä»ä¸­éšæœºæŠ½æ ·ï¼Œç”¨äºè®­ç»ƒç¥ç»ç½‘ç»œã€‚è¿™æ ·å¯ä»¥æ‰“ç ´æ•°æ®ä¹‹é—´çš„ç›¸å…³æ€§ï¼Œæé«˜è®­ç»ƒæ•ˆç‡ã€‚
  - **ç›®æ ‡ Q å€¼è®¡ç®—ï¼š** ä½¿ç”¨ Bellman æ–¹ç¨‹æ¥è®¡ç®—ç›®æ ‡ Q å€¼ï¼Œå³å½“å‰å¥–åŠ±åŠ ä¸Šä¸‹ä¸€çŠ¶æ€çš„æœ€å¤§ Q å€¼çš„æŠ˜æ‰£å€¼ã€‚è¿™ä¸ªç›®æ ‡ Q å€¼ç”¨äºè®­ç»ƒç½‘ç»œã€‚
  - **æŸå¤±å‡½æ•°ï¼š** ä½¿ç”¨ç›®æ ‡ Q å€¼å’Œç½‘ç»œè¾“å‡ºçš„ Q å€¼ä¹‹é—´çš„å‡æ–¹è¯¯å·®ä½œä¸ºæŸå¤±å‡½æ•°ï¼Œç”¨äºæ›´æ–°ç½‘ç»œå‚æ•°ã€‚
  - **æ¢¯åº¦ä¸‹é™ï¼š** ä½¿ç”¨æ¢¯åº¦ä¸‹é™ç®—æ³•æ¥æœ€å°åŒ–æŸå¤±å‡½æ•°ï¼Œæ›´æ–°ç½‘ç»œå‚æ•°ï¼Œä½¿ç½‘ç»œé€¼è¿‘æœ€ä¼˜çš„ Q å€¼å‡½æ•°ã€‚

  ç®—æ³•çš„å…³é”®ç‚¹ï¼š

  * ç”¨ç½‘ç»œé¢„æµ‹ Q å€¼ï¼Œå³ç½‘ç»œè¾“å…¥çŠ¶æ€ stateï¼Œè¾“å‡ºè¯¥çŠ¶æ€ä¸‹æ¯ä¸ªåŠ¨ä½œçš„ Q å€¼

  * Q å€¼çš„æ›´æ–°å˜ä¸ºç½‘ç»œå‚æ•°çš„æ›´æ–°ï¼Œå› æ­¤ç½‘ç»œçš„æŸå¤±å€¼å¯å®šä¹‰ä¸ºå‡æ–¹è¯¯å·®
    $$
    L = \cfrac12\big(Q(s, a) - (r + \gamma \max_{a'} Q(s', a'))\big)
    $$

  * æ¢ç´¢ä¸åˆ©ç”¨

    $\epsilon-greedy$ ä»¥æ¦‚ç‡ $\epsilon$ éšæœºé€‰æ‹©ä¸€ä¸ªåŠ¨ä½œï¼Œä»¥æ¦‚ç‡ $1-\epsilon$ é€‰æ‹©æœ€ä½³åŠ¨ä½œ

  * ç»éªŒå›æ”¾

    * ç”¨ å›æ”¾ç¼“å†²å­˜åŒº å¯ä»¥å‡å°‘ä¸ç¯å¢ƒåšäº’åŠ¨çš„ æ¬¡æ•°ï¼Œæé«˜è®­ç»ƒæ•ˆç‡
    * å‡å°‘åŒæ‰¹æ¬¡è®­ç»ƒæ•°æ®çš„ä¾èµ–å…³ç³»
    * åšæ³•ï¼šå°†æ¯ä¸€æ­¥è½¬ç§»çš„ çŠ¶æ€ã€åŠ¨ä½œã€å¥–åŠ±ã€ä¸‹ä¸€çŠ¶æ€ ç­‰ä¿¡æ¯ å­˜åˆ°ä¸€ä¸ªç¼“å†²åŒºï¼Œè®­ç»ƒç½‘ç»œæ—¶ ä»ç¼“å†²åŒºéšæœºæŠ½å–ä¸€æ‰¹æ•°æ®ä½œä¸ºè®­ç»ƒæ•°æ®

  * ç›®æ ‡ç½‘ç»œ

    * $(r + \gamma \max_{a'}Q(s', a'))$ å¯ä»¥çœ‹ä½œç›®æ ‡å€¼ï¼Œç›®æ ‡å€¼è·Ÿéš $Q$ ä¸€ç›´å˜åŒ–ä¼šç»™è®­ç»ƒå¸¦æ¥å›°éš¾
    * å°†è¯„ä¼°ç½‘ç»œä¸ç›®æ ‡ç½‘ç»œåˆ†å¼€ï¼Œç›®æ ‡ç½‘ç»œä¸è®­ç»ƒï¼Œè¯„ä¼°ç½‘ç»œæ›´æ–°è‹¥å¹²è½®åï¼Œç”¨è¯„ä¼°ç½‘ç»œå‚æ•°æ›¿æ¢ç›®æ ‡ç½‘ç»œå‚æ•°
    * $L = \cfrac12(Q_{eval}(s, a) - (r + \gamma \max_{a'} Q_{target} (s', a')))$ 

* é™¤äº†å®ç°åŸºæœ¬çš„ DQN ä¹‹å¤–ï¼Œæˆ‘è¿˜åœ¨è‡ªå·±å®ç°çš„åŸºæœ¬çš„DQNåŸºç¡€ä¹‹ä¸Šå°è¯•å®ç°äº†åŸºäº Dueling Network çš„ DQNã€åŸºäº Soft Update çš„ DQNã€‚

  * Dueling Networkï¼š

    Dueling Network æ˜¯ä¸€ç§æ”¹è¿›çš„æ·±åº¦ Q ç½‘ç»œï¼ˆDQNï¼‰ï¼Œç”¨äºå¼ºåŒ–å­¦ä¹ ä¸­çš„ä»·å€¼å‡½æ•°è¿‘ä¼¼ã€‚Dueling Network çš„ä¸»è¦æ€æƒ³æ˜¯é€šè¿‡å°†çŠ¶æ€å€¼ï¼ˆstate valueï¼‰å’Œä¼˜åŠ¿å‡½æ•°ï¼ˆadvantage functionï¼‰åˆ†å¼€ä¼°è®¡ï¼Œä»è€Œæé«˜è®­ç»ƒæ•ˆç‡å’Œæ€§èƒ½ã€‚

    1. èƒŒæ™¯

       åœ¨ä¼ ç»Ÿçš„ DQN ä¸­ï¼Œä½¿ç”¨ä¸€ä¸ªç¥ç»ç½‘ç»œæ¥è¿‘ä¼¼ Q å‡½æ•°ï¼Œå³ç»™å®šä¸€ä¸ªçŠ¶æ€$s$ å’Œä¸€ä¸ªåŠ¨ä½œ$a$ï¼Œé¢„æµ‹å…¶ Q å€¼$Q(s, a)$ã€‚ç„¶è€Œï¼Œè¿™ç§æ–¹æ³•åœ¨æŸäº›æƒ…å†µä¸‹å¹¶ä¸é«˜æ•ˆï¼Œç‰¹åˆ«æ˜¯åœ¨ä¸€äº›çŠ¶æ€ä¸‹ï¼ŒåŠ¨ä½œçš„é€‰æ‹©å¯¹ Q å€¼å½±å“ä¸å¤§ï¼Œå³åŠ¨ä½œä¹‹é—´çš„å·®å¼‚ä¸æ˜æ˜¾ã€‚

    2. Dueling Network çš„ç»“æ„

       Dueling Network é€šè¿‡å¼•å…¥ä¸¤ä¸ªåˆ†æ”¯ç½‘ç»œæ¥è§£å†³ä¸Šè¿°é—®é¢˜ï¼Œä¸€ä¸ªç”¨äºä¼°è®¡çŠ¶æ€å€¼$V(s)$ï¼Œå¦ä¸€ä¸ªç”¨äºä¼°è®¡ä¼˜åŠ¿å‡½æ•°$A(s, a)$ã€‚

       å…·ä½“æ¥è¯´ï¼Œä¸€ä¸ªç¥ç»ç½‘ç»œåœ¨æœ€åçš„å‡ å±‚ä¹‹å‰æ˜¯å…±äº«çš„ï¼Œç„¶ååˆ†æˆä¸¤ä¸ªåˆ†æ”¯ï¼š
       1. çŠ¶æ€å€¼åˆ†æ”¯ï¼šä¼°è®¡çŠ¶æ€å€¼$V(s)$ï¼Œå³çŠ¶æ€æœ¬èº«çš„ä»·å€¼ã€‚
       2. ä¼˜åŠ¿å‡½æ•°åˆ†æ”¯ï¼šä¼°è®¡ä¼˜åŠ¿å‡½æ•°$A(s, a)$ï¼Œå³åœ¨ç‰¹å®šçŠ¶æ€ä¸‹ï¼Œé€‰æ‹©ç‰¹å®šåŠ¨ä½œç›¸æ¯”å…¶ä»–åŠ¨ä½œçš„ä¼˜åŠ¿ã€‚

       æœ€åï¼Œè¿™ä¸¤ä¸ªåˆ†æ”¯çš„è¾“å‡ºç»„åˆæˆ Q å€¼

    3. ç‰¹ç‚¹ï¼š

       ä¸ä¼ ç»Ÿ DQN ç›¸æ¯”ï¼ŒDueling Network DQN çš„ä¸»è¦åŒºåˆ«åœ¨äº Q ç½‘ç»œçš„æ¶æ„è®¾è®¡ã€‚å…¶ä»–éƒ¨åˆ†ï¼ŒåŒ…æ‹¬ç»éªŒå›æ”¾ã€ç›®æ ‡ç½‘ç»œæ›´æ–°ã€æŸå¤±å‡½æ•°ç­‰ï¼Œéƒ½ä¸ DQN ç±»ä¼¼ã€‚

  * è½¯æ›´æ–°ï¼ˆSoft Updateï¼‰ï¼š

    åœ¨æ·±åº¦ Q ç½‘ç»œï¼ˆDQNï¼‰ä¸­ï¼Œç›®æ ‡ç½‘ç»œï¼ˆtarget networkï¼‰çš„å¼•å…¥æ˜¯ä¸ºäº†æé«˜è®­ç»ƒçš„ç¨³å®šæ€§ã€‚ç„¶è€Œï¼Œç›®æ ‡ç½‘ç»œå‚æ•°çš„æ›´æ–°é¢‘ç‡ä¹Ÿä¼šå¯¹è®­ç»ƒè¿‡ç¨‹äº§ç”Ÿæ˜¾è‘—å½±å“ã€‚ä¸ºäº†åœ¨ç¨³å®šæ€§å’Œæ•ˆç‡ä¹‹é—´æ‰¾åˆ°å¹³è¡¡ï¼ŒåŸºäº Soft Update çš„ DQN æä¾›äº†ä¸€ç§æ¸è¿›æ›´æ–°ç›®æ ‡ç½‘ç»œå‚æ•°çš„æ–¹æ³•ã€‚

    1. èƒŒæ™¯

       åœ¨ä¼ ç»Ÿçš„ DQN ä¸­ï¼Œç›®æ ‡ç½‘ç»œçš„å‚æ•°æ¯éš”ä¸€å®šçš„æ­¥æ•°ï¼ˆä¾‹å¦‚ï¼Œæ¯1000æ­¥ï¼‰æ‰ä»å½“å‰ Q ç½‘ç»œå¤åˆ¶ä¸€æ¬¡ã€‚è™½ç„¶è¿™ç§ç¡¬æ€§æ›´æ–°ï¼ˆhard updateï¼‰ç­–ç•¥èƒ½ç¨³å®šè®­ç»ƒè¿‡ç¨‹ï¼Œä½†å®ƒåœ¨å‚æ•°æ›´æ–°çš„é—´éš”ä¹‹é—´å¯èƒ½ä¼šå¯¼è‡´ç›®æ ‡ Q å€¼å˜åŒ–è¿‡å¤§ï¼Œå½±å“æ”¶æ•›é€Ÿåº¦ã€‚

    2. Soft Update çš„æ¦‚å¿µ

       Soft Update æ˜¯ä¸€ç§å¯¹ç›®æ ‡ç½‘ç»œå‚æ•°è¿›è¡Œæ¸è¿›æ›´æ–°çš„æ–¹æ³•ï¼Œè€Œä¸æ˜¯æ¯éš”å›ºå®šæ­¥æ•°è¿›è¡Œä¸€æ¬¡å¤§è§„æ¨¡çš„å‚æ•°å¤åˆ¶ã€‚å…·ä½“æ¥è¯´ï¼ŒSoft Update é€šè¿‡æŒ‡æ•°åŠ æƒå¹³å‡çš„æ–¹æ³•ï¼Œå°†ç›®æ ‡ç½‘ç»œå‚æ•°é€æ­¥å‘å½“å‰ Q ç½‘ç»œå‚æ•°é æ‹¢ã€‚

    3. Soft Update çš„å…¬å¼

       åœ¨ Soft Update ä¸­ï¼Œç›®æ ‡ç½‘ç»œå‚æ•° $\theta^-$ æ›´æ–°ä¸ºï¼š
       $\theta' \leftarrow \tau \theta + (1 - \tau) \theta'$
       å…¶ä¸­ï¼š

       - $\theta$ æ˜¯å½“å‰ Q ç½‘ç»œçš„å‚æ•°ã€‚

       - $\theta'$ æ˜¯ç›®æ ‡ç½‘ç»œçš„å‚æ•°ã€‚

       - $\tau$ æ˜¯ä¸€ä¸ªå¾ˆå°çš„å¸¸æ•°ï¼Œç§°ä¸ºè½¯æ›´æ–°ç³»æ•°ï¼ˆsoft update coefficientï¼‰ã€‚
    
    4. ç‰¹ç‚¹ï¼šæ›´å¹³æ»‘çš„æ›´æ–°ï¼šç”±äºç›®æ ‡ç½‘ç»œå‚æ•°çš„æ›´æ–°æ˜¯é€æ­¥è¿›è¡Œçš„ï¼ŒSoft Update èƒ½å¤Ÿæä¾›æ›´å¹³æ»‘çš„ Q å€¼å˜åŒ–ï¼Œå‡å°‘è®­ç»ƒè¿‡ç¨‹ä¸­ä¸ç¨³å®šçš„éœ‡è¡ã€‚
    




### 2.ä¼ªä»£ç 

ï¼ˆä¸­æ–‡ç‰ˆä¼ªä»£ç ï¼‰

**ç®—æ³•**ï¼šå…·æœ‰ç»éªŒå›æ”¾çš„æ·±åº¦ Q å­¦ä¹ 
$$
\begin{aligned}
&\text{åˆå§‹åŒ– replay memory }D\text{ ä¸º capacity }N \\
&\text{åˆå§‹åŒ– åŠ¨ä½œä»·å€¼å‡½æ•° }Q\text{ ä¸º éšæœºæƒé‡ }\theta\\
&\text{åˆå§‹åŒ– ç›®æ ‡åŠ¨ä½œä»·å€¼å‡½æ•° }\hat{Q}\text{ ä¸ºæƒé‡ }\theta^- = \theta\\
&\bold{For} \text{ episode} = 1, M\ \bold{do} \\
&\ \ \ \ \text{åˆå§‹åŒ– åºåˆ— } s_1 = {x_1} \text{ å’Œé¢„å¤„ç†åºåˆ— } \phi_1 = \phi_1(s_1) \\
&\ \ \ \ \bold{For}\ t=1,\text{T}\ \bold{do}\\
&\ \ \ \ \ \ \ \ \text{ä»¥æ¦‚ç‡ }\varepsilon\text{ é€‰æ‹©ä¸€ä¸ªéšæœºåŠ¨ä½œ }a_t\\
&\ \ \ \ \ \ \ \ \text{å¦åˆ™é€‰æ‹© } a_t = \mathrm{argmax}_aQ(\phi(s_t), a; \theta)\\
&\ \ \ \ \ \ \ \ \text{åœ¨æ¨¡æ‹Ÿå™¨ä¸­æ‰§è¡ŒåŠ¨ä½œ} a_t \text{è§‚å¯Ÿ reward } r_t \text{ å’Œå›¾åƒ } x_{t+1} \\
&\ \ \ \ \ \ \ \ \text{ä»¤ } s_{t+1} = s_t, a_t, x_{t+1} \text{ ä»¥åŠé¢„å¤„ç†} \phi_{t+1} = \phi(s_{t+1}) \\
&\ \ \ \ \ \ \ \ \text{åœ¨ }D\text{ ä¸­ä¿å­˜ transitions }(\phi_j, a_j, r_j, \phi_{j+1})\\
&\ \ \ \ \ \ \ \ \text{åœ¨ }D\text{ ä¸­éšæœºé‡‡æ ·ä¸€å°æ‰¹ transitions }(\phi_t, a_t, r_t, \phi_{t+1})\\
&\ \ \ \ \ \ \ \ è®¾ç½® y_j = \begin{cases}r_j&  \text{å¦‚æœ episode ç»ˆæ­¢åœ¨j+1æ­¥}\\
r_j + \gamma \max_{a'} \hat{Q}(\phi_{j+1}, a'; \theta^-) & \text{å…¶ä»–æƒ…å†µ}
\end{cases}\\
&\ \ \ \ \ \ \ \ \text{å¯¹ç½‘ç»œå‚æ•°}\theta\text{ è¿›è¡Œæ¢¯åº¦ä¸‹é™æ“ä½œï¼Œä»¥ä¼˜åŒ–}(y_j - Q(\phi_j, a_j; \theta))^2\text{ çš„å€¼}\\
&\ \ \ \ \ \ \ \ \text{æ¯ }C \text{ æ­¥é‡ç½® } \hat{Q} = Q\\
&\ \ \ \ \bold{End\ For}\\
& \bold{End\ For}
\end{aligned}
$$



### 3.å…³é”®ä»£ç å±•ç¤º

#### DQN

è®¾è®¡ QNetï¼š

```python
class QNet(nn.Module):
    """
    ä¸€ä¸ªç”¨äºè¡¨ç¤ºæ·±åº¦ Q ç½‘ç»œ (DQN) ä¸­çš„ Q å€¼å‡½æ•°çš„ç¥ç»ç½‘ç»œæ¶æ„ã€‚

    è¯¥ç½‘ç»œæ¥æ”¶ç¯å¢ƒçš„è¾“å…¥çŠ¶æ€ï¼Œé€šè¿‡éšè—å±‚è¿›è¡Œå¤„ç†ï¼Œå¹¶è¾“å‡ºæ¯ä¸ªå¯ç”¨åŠ¨ä½œçš„ä¼°è®¡ Q å€¼ã€‚

    å±æ€§:
        input_size (int): è¾“å…¥çŠ¶æ€çš„ç»´åº¦ã€‚
        hidden_size (int): éšè—å±‚çš„ç¥ç»å…ƒæ•°é‡ã€‚
        output_size (int): è¾“å‡ºç¥ç»å…ƒçš„æ•°é‡ï¼Œä»£è¡¨åŠ¨ä½œçš„ä¸ªæ•°ã€‚
        fc1 (nn.Linear): å¤„ç†è¾“å…¥çŠ¶æ€çš„ç¬¬ä¸€ä¸ªå…¨è¿æ¥çº¿æ€§å±‚ã€‚
        fc2 (nn.Linear): ç”Ÿæˆ Q å€¼ä¼°è®¡çš„ç¬¬äºŒä¸ªå…¨è¿æ¥çº¿æ€§å±‚ã€‚
    """

    def __init__(self, input_size, hidden_size, output_size):
        super(QNet, self).__init__()
        self.fc1 = nn.Linear(input_size, hidden_size)  # åˆ›å»ºç¬¬ä¸€ä¸ªçº¿æ€§å±‚
        self.fc2 = nn.Linear(hidden_size, output_size)  # åˆ›å»ºç¬¬äºŒä¸ªçº¿æ€§å±‚

    def forward(self, x):
        x = torch.Tensor(x)  # å°†è¾“å…¥è½¬æ¢ä¸º PyTorch å¼ é‡
        x = F.relu(self.fc1(x))  # å¯¹ç¬¬ä¸€ä¸ªçº¿æ€§å±‚çš„è¾“å‡ºåº”ç”¨ ReLU æ¿€æ´»å‡½æ•°
        x = self.fc2(x)  # å°†å¤„ç†åçš„æ•°æ®ä¼ é€’åˆ°ç¬¬äºŒä¸ªçº¿æ€§å±‚
        return x
```



ç»éªŒå›æ”¾ç¼“å†²åŒºï¼š

```python
# ç»éªŒå›æ”¾ç¼“å†²åŒº
class ReplayBuffer:
    def __init__(self, capacity):
        self.buffer = deque(maxlen=capacity)

    def len(self):
        return len(self.buffer)

    def push(self, *transition):
        self.buffer.append(transition)

    def sample(self, batch_size):
        transitions = random.sample(self.buffer, batch_size)
        obs, actions, rewards, next_obs, dones = zip(*transitions)  # è§£å‹ç¼©å…ƒç»„
        return np.array(obs), actions, rewards, np.array(next_obs), dones

    def clean(self):
        self.buffer.clear()
```



å›ºå®š epsilon çš„ DQN çš„å®ç°ï¼š

```python
class DQN:
    def __init__(self, env, input_size, hidden_size, output_size):
        self.env = env
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        # è¯„ä¼°ç½‘ç»œ
        self.eval_net = QNet(input_size, hidden_size, output_size).to(self.device)
        # ç›®æ ‡ç½‘ç»œ
        self.target_net = QNet(input_size, hidden_size, output_size).to(self.device)
        # Adamä¼˜åŒ–å™¨
        self.optim = optim.Adam(self.eval_net.parameters(), lr=args.lr)
        # Îµ-è´ªå¿ƒç­–ç•¥çš„å‚æ•° Îµ éšç€æ¸¸æˆå›åˆçš„å¢åŠ ï¼Œé€æ¸å‡å°‘
        # åœ¨è®­ç»ƒå¼€å§‹æ—¶ï¼Œæ™ºèƒ½ä½“æ›´å¤šåœ°è¿›è¡Œæ¢ç´¢ï¼ˆå³æœ‰è¾ƒé«˜çš„æ¦‚ç‡é€‰æ‹©éšæœºåŠ¨ä½œï¼‰
        # åœ¨è®­ç»ƒå¿«è¦ç»“æŸæ—¶ï¼Œæ™ºèƒ½ä½“æ›´å¤šåœ°è¿›è¡Œåˆ©ç”¨ï¼ˆå³æœ‰è¾ƒä½çš„æ¦‚ç‡é€‰æ‹©éšæœºåŠ¨ä½œï¼‰
        self.eps = args.eps
        # åˆ›å»ºç»éªŒå›æ”¾ç¼“å†²åŒº
        self.buffer = ReplayBuffer(args.capacity)
        self.loss_fn = nn.MSELoss()
        self.learn_step = 0
        self.steps_done = 0

    # é€‰æ‹©åŠ¨ä½œ - ä½¿ç”¨Îµ-è´ªå¿ƒç­–ç•¥
    def choose_action(self, obs):
        # Îµ çš„æ¦‚ç‡è¿›è¡Œæ¢ç´¢ï¼ˆéšæœºé€‰æ‹©ä¸€ä¸ªåŠ¨ä½œï¼‰
        # 1-Îµ çš„æ¦‚ç‡é€‰æ‹©æœ€ä½³åŠ¨ä½œ
        self.steps_done += 1
        if np.random.rand() < self.eps:
            # ç”Ÿæˆ0åˆ°1ä¹‹é—´çš„éšæœºæ•°ï¼Œå¦‚æœå°äºÎµï¼Œåˆ™è¿›è¡Œæ¢ç´¢ï¼Œé€‰æ‹©éšæœºåŠ¨ä½œ
            return self.env.action_space.sample()
        # 1-Îµ çš„æ¦‚ç‡ï¼š æ ¹æ®å½“å‰ç­–ç•¥é€‰æ‹©æœ€ä¼˜åŠ¨ä½œ
        obs = torch.Tensor(obs).unsqueeze(0).to(self.device)
        with torch.no_grad():  # å…³é—­æ¢¯åº¦è®¡ç®—
            actions = self.eval_net(obs)
        return torch.argmax(actions).item()  # è¿”å› Q å€¼æœ€å¤§çš„åŠ¨ä½œ

    def store_transition(self, *transition):
        self.buffer.push(*transition)

    def learn(self):
        if self.learn_step % args.update_target == 0:  # å®šæœŸæ›´æ–°ç›®æ ‡ç½‘ç»œ
            self.target_net.load_state_dict(self.eval_net.state_dict())
        self.learn_step += 1

        # ä»ç¼“å†²åŒºé‡‡æ ·ä¸€ä¸ªæ‰¹æ¬¡çš„ç»éªŒ
        obs, actions, rewards, next_obs, dones = self.buffer.sample(args.batch_size)
        # è½¬æ¢ä¸º pytorch å¼ é‡
        obs = torch.Tensor(obs).to(self.device)
        actions = torch.LongTensor(actions).unsqueeze(1).to(self.device)
        rewards = torch.FloatTensor(rewards).unsqueeze(1).to(self.device)
        next_obs = torch.Tensor(next_obs).to(self.device)
        dones = torch.FloatTensor(dones).unsqueeze(1).to(self.device)

        # è®¡ç®—è¯„ä¼°ç½‘ç»œçš„Qå€¼
        q_eval = self.eval_net(obs).gather(1, actions)
        # è®¡ç®—ç›®æ ‡ç½‘ç»œçš„Qå€¼
        q_next = self.target_net(next_obs).max(1)[0].detach().unsqueeze(1)
        q_target = rewards + args.gamma * q_next * (1 - dones)

        # è®¡ç®—æŸå¤±
        loss = self.loss_fn(q_eval, q_target)

        # åå‘ä¼ æ’­å’Œä¼˜åŒ–
        self.optim.zero_grad()
        loss.backward()
        self.optim.step()
```





é€æ­¥ä¸‹é™epsilon çš„ DQN å®ç°ï¼š

å°†å›ºå®šçš„ $\varepsilon$ æ›¿æ¢æˆéšç€è¿­ä»£æ¬¡æ•°é€æ­¥é™ä½çš„ $\varepsilon$ ã€‚åœ¨è¿›è¡Œè¾ƒå°‘å›åˆæ•°çš„æ—¶å€™ï¼Œ $\varepsilon$ è¾ƒå¤§ï¼Œæ›´æœ‰å¯èƒ½åšå‡ºéšæœºåŠ¨ä½œï¼Œè¿›è¡Œæ¢ç´¢ï¼Œåˆ°äº†è¾ƒå¤šå›åˆæ•°çš„æ—¶å€™ï¼Œ $\varepsilon$â€‹ è¾ƒå°ï¼Œåšå‡ºè¡ŒåŠ¨å†³å®šçš„æ—¶å€™æ›´å¯èƒ½ä¼šé€‰æ‹©æ ¹æ®ç­–ç•¥å¾—å‡ºçš„æœ€ä¼˜åŠ¨ä½œã€‚è¿™æ ·å®ç°çš„ DQNï¼Œå¯èƒ½å¯ä»¥æ›´å¿«åœ°å¾—åˆ°æœ€ä¼˜æ•ˆæœã€‚

ä¸»è¦åœ¨ä¸Šé¢ç¨‹åºçš„åŸºç¡€ä¸Šä¿®æ”¹äº† `__init__` å‡½æ•°å’Œ`choose_action` å‡½æ•°ï¼š

```python
# Deep Q-Learning Network
class DQN:
    def __init__(self, env, input_size, hidden_size, output_size):
        self.env = env
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

        # è¯„ä¼°ç½‘ç»œ
        self.eval_net = QNet(input_size, hidden_size, output_size).to(self.device)
        # ç›®æ ‡ç½‘ç»œ
        self.target_net = QNet(input_size, hidden_size, output_size).to(self.device)

        # Adamä¼˜åŒ–å™¨
        self.optim = optim.Adam(self.eval_net.parameters(), lr=args.lr)

        # Îµ-è´ªå¿ƒç­–ç•¥çš„å‚æ•° Îµ éšç€æ¸¸æˆå›åˆçš„å¢åŠ ï¼Œé€æ¸å‡å°‘
        # åœ¨è®­ç»ƒå¼€å§‹æ—¶ï¼Œæ™ºèƒ½ä½“æ›´å¤šåœ°è¿›è¡Œæ¢ç´¢ï¼ˆå³æœ‰è¾ƒé«˜çš„æ¦‚ç‡é€‰æ‹©éšæœºåŠ¨ä½œï¼‰
        # åœ¨è®­ç»ƒå¿«è¦ç»“æŸæ—¶ï¼Œæ™ºèƒ½ä½“æ›´å¤šåœ°è¿›è¡Œåˆ©ç”¨ï¼ˆå³æœ‰è¾ƒä½çš„æ¦‚ç‡é€‰æ‹©éšæœºåŠ¨ä½œï¼‰
        self.eps_start = args.eps_start
        self.eps_end = args.eps_end
        # Îµ ä¼šåœ¨çº¦ eps_decay ä¸ªæ­¥éª¤å†…ä» eps_start çº¿æ€§è¡°å‡åˆ°æ¥è¿‘ eps_end
        self.eps_decay = args.eps_decay

        # åˆ›å»ºç»éªŒå›æ”¾ç¼“å†²åŒº
        self.buffer = ReplayBuffer(args.capacity)

        self.loss_fn = nn.MSELoss()
        self.learn_step = 0
        self.steps_done = 0

    # é€‰æ‹©åŠ¨ä½œ - ä½¿ç”¨Îµ-è´ªå¿ƒç­–ç•¥
    def choose_action(self, obs):
        # Îµ çš„æ¦‚ç‡è¿›è¡Œæ¢ç´¢ï¼ˆéšæœºé€‰æ‹©ä¸€ä¸ªåŠ¨ä½œï¼‰
        # 1-Îµ çš„æ¦‚ç‡é€‰æ‹©æœ€ä½³åŠ¨ä½œ
        epsilon = self.eps_end + (self.eps_start - self.eps_end) * np.exp(-self.steps_done / self.eps_decay)
        self.steps_done += 1
        if np.random.rand() < epsilon:
            # ç”Ÿæˆ0åˆ°1ä¹‹é—´çš„éšæœºæ•°ï¼Œå¦‚æœå°äºÎµï¼Œåˆ™è¿›è¡Œæ¢ç´¢ï¼Œé€‰æ‹©éšæœºåŠ¨ä½œ
            return self.env.action_space.sample()
        # 1-Îµ çš„æ¦‚ç‡ï¼š æ ¹æ®å½“å‰ç­–ç•¥é€‰æ‹©æœ€ä¼˜åŠ¨ä½œ
        obs = torch.Tensor(obs).unsqueeze(0).to(self.device)
        with torch.no_grad():  # å…³é—­æ¢¯åº¦è®¡ç®—
            actions = self.eval_net(obs)
        return torch.argmax(actions).item()  # è¿”å› Q å€¼æœ€å¤§çš„åŠ¨ä½œ
```



ä¿®æ”¹ `main()` å‡½æ•°ï¼š

```python
def main():
    # åˆ›å»ºç¯å¢ƒ
    env = gym.make(args.env)
    # è·å–ç¯å¢ƒçš„çŠ¶æ€å’ŒåŠ¨ä½œç©ºé—´ç»´åº¦
    o_dim = env.observation_space.shape[0]
    a_dim = env.action_space.n
    # åˆ›å»ºDQNæ™ºèƒ½ä½“
    agent = DQN(env, o_dim, args.hidden, a_dim)
    rewards = []      # å­˜å‚¨æ¯ä¸ªæ¸¸æˆå›åˆçš„å¥–åŠ±
    avg_rewards = []  # å­˜å‚¨æ¯ 100 ä¸ªå›åˆçš„å¹³å‡å¥–åŠ±

    # è¿›è¡Œè®­ç»ƒ
    for i_episode in range(args.n_episodes):
        obs = env.reset()     # æ¸¸æˆé‡ç½®
        episode_reward = 0    # å¥–åŠ±
        done = False          # æ¸¸æˆæ˜¯å¦ç»“æŸ
        step_cnt = 0          # æ‰§è¡Œæ­¥éª¤çš„æ¬¡æ•°
        while not done and step_cnt < 500:
            step_cnt += 1
            env.render()      # æ¸¸æˆç”»é¢æ¸²æŸ“
            action = agent.choose_action(obs)  # é€‰æ‹©åŠ¨ä½œå¹¶æ‰§è¡Œ
            next_obs, reward, done, info = env.step(action)  # æ‰§è¡ŒåŠ¨ä½œ
            # å­˜å‚¨ç»éªŒ
            agent.store_transition(obs, action, reward, next_obs, done)
            episode_reward += reward  # æ¸¸æˆå›åˆçš„å¥–åŠ±
            obs = next_obs   # æ›´æ–°çŠ¶æ€
            # å¦‚æœç¼“å†²åŒºä¸­çš„ç»éªŒæ•°é‡è¶³å¤Ÿï¼Œåˆ™è¿›è¡Œå­¦ä¹ 
            if agent.buffer.len() >= args.batch_size:
                agent.learn()

        print(f"Episode: {i_episode}, Reward: {episode_reward}")
        rewards.append(episode_reward)
        avg_rewards.append(np.mean(rewards[-100:]))

    # ç»˜åˆ¶å¥–åŠ±æ›²çº¿
    plt.figure()
    plt.plot(rewards)
    plt.xlabel('Episode')
    plt.ylabel('Reward')
    plt.title('Reward Over Episodes')

    plt.figure()
    plt.plot(avg_rewards)
    plt.ylim(0, 500)
    plt.yticks(range(0, 501, 25))
    plt.xlabel('Episode')
    plt.ylabel('Average Reward')
    plt.title('Average Reward Per 100 Episodes')
    plt.show()
```



ä¿®æ”¹è¾“å…¥å‚æ•°ï¼š

```python
if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--env", default="CartPole-v1", type=str, help="environment name")
    parser.add_argument("--lr", default=1e-3, type=float, help="learning rate")
    parser.add_argument("--hidden", default=64, type=int, help="dimension of hidden layer")
    parser.add_argument("--n_episodes", default=500, type=int, help="number of episodes")
    parser.add_argument("--gamma", default=0.99, type=float, help="discount factor")
    parser.add_argument("--capacity", default=10000, type=int, help="capacity of replay buffer")
    parser.add_argument("--batch_size", default=128, type=int)
    parser.add_argument("--update_target", default=100, type=int, help="frequency to update target network")
    parser.add_argument("--eps_start", default=0.9, type=float)
    parser.add_argument("--eps_end", default=0.01, type=float)
    parser.add_argument("--eps_decay", default=500, type=int)
    args = parser.parse_args()
    main()
```

ä¸Šé¢æ˜¯åŸºç¡€çš„ DQN çš„å®ç°ï¼Œé™¤äº†ä¸Šé¢çš„å®éªŒå¤–ï¼Œæˆ‘è¿˜åœ¨ä¸Šé¢çš„DQNåŸºç¡€ä¹‹ä¸Šå®ç°äº†åŸºäº Dueling Network çš„ DQNã€åŸºäº Soft Update çš„ DQNã€‚



#### åŸºäº Dueling Network çš„ DQN

è¿™ä¸ªä¹Ÿæ˜¯ä¿®æ”¹ QNet çš„ç»“æ„ï¼Œä¸»è¦ä¿®æ”¹åœ¨äºè¯„ä¼°åŠ¨ä½œçš„æ–¹å¼å‘ç”Ÿæ”¹å˜ã€‚è¯¥æ¶æ„å°† DQN çš„æœ€åä¸€å±‚æ‹†åˆ†ä¸ºä¸¤ä¸ªåˆ†æ”¯ã€‚ä¸€ä¸ªåˆ†æ”¯ä¼°è®¡å½“å‰çŠ¶æ€çš„ä»·å€¼ (V(s))ï¼Œç‹¬ç«‹äºä»»ä½•ç‰¹å®šåŠ¨ä½œã€‚å¦ä¸€ä¸ªåˆ†æ”¯ä¼°è®¡æ¯ä¸ªåŠ¨ä½œç›¸å¯¹äºè¯¥çŠ¶æ€çš„å¹³å‡ä¼˜åŠ¿ (A(s,a))ã€‚ä¼ ç»Ÿ DQN è¾“å‡º Q å€¼ï¼Œè¡¨ç¤ºåœ¨ç»™å®šçŠ¶æ€ä¸‹é‡‡å–ç‰¹å®šåŠ¨ä½œçš„é¢„æœŸå¥–åŠ±ã€‚è€Œåœ¨ Dueling DQN ä¸­ï¼Œæ¯ä¸ªåŠ¨ä½œçš„æœ€ç»ˆ Q å€¼æ˜¯é€šè¿‡å°†ä¼°è®¡å€¼ (V(s)) ä¸è¯¥åŠ¨ä½œçš„ä¼˜åŠ¿ (A(s,a)) è¿›è¡Œç»„åˆå¾—åˆ°ã€‚

ä¿®æ”¹åçš„ QNet ç»“æ„ï¼š

```python
class DuelingQNet(nn.Module):
    """Dueling Qç½‘ç»œæ¶æ„

    è¯¥ç±»å®šä¹‰äº†Dueling Qç½‘ç»œçš„ç»“æ„ã€‚

    Attributes:
        fc1 (nn.Linear): ç¬¬ä¸€ä¸ªå…¨è¿æ¥å±‚ï¼Œå°†è¾“å…¥æ˜ å°„åˆ°éšè—å±‚ã€‚
        fc_value (nn.Linear): ä»·å€¼ç½‘ç»œçš„è¾“å‡ºå±‚ï¼Œç”¨äºé¢„æµ‹çŠ¶æ€çš„æ•´ä½“ä»·å€¼ã€‚
        fc_advantage (nn.Linear): ä¼˜åŠ¿ç½‘ç»œçš„è¾“å‡ºå±‚ï¼Œç”¨äºé¢„æµ‹æ¯ä¸ªåŠ¨ä½œç›¸å¯¹äºå¹³å‡åŠ¨ä½œçš„ä¼˜åŠ¿ã€‚
    """

    def __init__(self, input_size, hidden_size, output_size):
        super(DuelingQNet, self).__init__()
        self.fc1 = nn.Linear(input_size, hidden_size)
        self.fc_value = nn.Linear(hidden_size, 1)
        self.fc_advantage = nn.Linear(hidden_size, output_size)

    def forward(self, x):
        x = torch.Tensor(x)
        x = F.relu(self.fc1(x))   # æ¿€æ´»å‡½æ•° ReLU
        value = self.fc_value(x)  # ä»·å€¼ç½‘ç»œçš„è¾“å‡º
        advantage = self.fc_advantage(x)  # ä¼˜åŠ¿ç½‘ç»œçš„è¾“å‡º
        q_values = value + (advantage - advantage.mean(dim=1, keepdim=True))  # è®¡ç®— Q å€¼
        return q_values
```



#### åŸºäºè½¯æ›´æ–°çš„ DQN

ä¸»è¦æ˜¯åŠ å…¥ä¸€ä¸ªå‚æ•° $\tau$ ï¼Œä½œä¸ºæ¯æ¬¡æ›´æ–°ç½‘ç»œçš„æƒé‡ã€‚å¸¦æœ‰è½¯æ›´æ–°çš„ç›®æ ‡ç½‘ç»œçš„æ›´æ–°å…¬å¼ä¸ºï¼š
$$
\theta' = \tau \theta + (1-\tau)\theta'
$$
$\theta$ è¡¨ç¤ºä¸»ç½‘ç»œçš„æƒé‡ï¼Œ$\theta'$ è¡¨ç¤ºç›®æ ‡ç½‘ç»œçš„æƒé‡ã€‚

åŠ å…¥è½¯æ›´æ–°çš„ DQNçš„è¿­ä»£æ›´æ–°çš„æ›²çº¿ä¼šæ›´åŠ å¹³ç¨³ã€‚

ä¸»è¦ä¿®æ”¹çš„åœ°æ–¹åœ¨  `learn` å‡½æ•°ä¸Šï¼Œä¿®æ”¹äº†æ›´æ–°ç½‘ç»œçš„ä»£ç ï¼š

```python
    def soft_update_target_network(self):
        for target_param, eval_param in zip(self.target_net.parameters(), self.eval_net.parameters()):
            target_param.data.copy_(self.tau * eval_param.data + (1.0 - self.tau) * target_param.data)

    def learn(self):
        self.soft_update_target_network()  # æ¯æ¬¡éƒ½è¿›è¡Œè½¯æ›´æ–°

        # ä»ç¼“å†²åŒºé‡‡æ ·ä¸€ä¸ªæ‰¹æ¬¡çš„ç»éªŒ
        obs, actions, rewards, next_obs, dones = self.buffer.sample(args.batch_size)
        # ...  # å…¶ä»–ä»£ç å’Œä¹‹å‰çš„ä¸€æ ·
```

ä¿®æ”¹è¾“å…¥çš„å‚æ•°ï¼ŒåŠ å…¥ä¸€ä¸ª $\tau$ å‚æ•°ï¼š

```python
if __name__ == "__main__":
    # å…¶ä»–å‚æ•°å’Œ é€æ­¥ä¸‹é™epsilonçš„DQNä¸€æ ·
    # åŠ å…¥ å‚æ•° tau
    parser.add_argument("--tau", default=0.01, type=float, help="soft update parameter")
    args = parser.parse_args()
    main()
```



### 4.åˆ›æ–°ç‚¹&ä¼˜åŒ–

* åˆ©ç”¨æ ¹æ®æ—¶é—´æ­¥çš„ $\varepsilon$ ï¼Œä½¿å¾—æ¨¡å‹åœ¨å‰æœŸè¿›è¡Œå¤§é‡æ¢ç´¢ï¼Œèƒ½å¤Ÿä¸€å®šç¨‹åº¦ä¸ŠåŠ å¿«æ¨¡å‹è®­ç»ƒçš„é€Ÿåº¦ã€‚åŒæ—¶åœ¨åæœŸï¼Œæ›´å€¾å‘äºé€‰æ‹©æ ¹æ®å†³ç­–æ¨¡å‹å¾—åˆ°çš„è¡ŒåŠ¨ã€‚
* å°è¯•ä½¿ç”¨åŸºäº Dueling Network çš„ DQN å¯¹å›¾åƒè¿›è¡Œå¤„ç†ã€‚
* æ ¹æ®è‡ªå·±è®¾è®¡åŸæ¨¡å‹çš„ä¸ç¨³å®šï¼Œæ‰€ä»¥å°è¯•ä½¿ç”¨ Soft Update ä¿®æ”¹æ¨¡å‹æ›´æ–°éƒ¨åˆ†çš„ä»£ç ã€‚



## ä¸‰ã€å®éªŒç»“æœåŠåˆ†æ

é€æ­¥ä¸‹é™ $\epsilon$ çš„ DQN çš„æµ‹è¯•ç»“æœï¼š

![image-20240606214215406](å®éªŒæŠ¥å‘Š/image-20240606214215406.png)

<img src="å®éªŒæŠ¥å‘Š/image-20240606214137020.png" alt="image-20240606214137020" style="zoom:50%;" /><img src="å®éªŒæŠ¥å‘Š/image-20240606214056083.png" alt="image-20240606214056083" style="zoom:50%;" />

å›ºå®š $\epsilon$ çš„ DQN çš„æµ‹è¯•ç»“æœï¼šï¼ˆ$\epsilon = 0.1$ï¼‰

<img src="å®éªŒæŠ¥å‘Š/image-20240607103351496.png" alt="image-20240607103351496" style="zoom:50%;" /><img src="å®éªŒæŠ¥å‘Š/image-20240607103433843.png" alt="image-20240607103433843" style="zoom:50%;" />



åŸºäº Dueling Network çš„ DQN:

<img src="å®éªŒæŠ¥å‘Š/image-20240607005154848.png" alt="image-20240607005154848" style="zoom:50%;" /><img src="å®éªŒæŠ¥å‘Š/image-20240607005141921.png" alt="image-20240607005141921" style="zoom:50%;" />



åŸºäºè½¯æ›´æ–°çš„DQNï¼š

<img src="å®éªŒæŠ¥å‘Š/image-20240606225608401.png" alt="image-20240606225608401" style="zoom:50%;" /><img src="å®éªŒæŠ¥å‘Š/image-20240606225546330.png" alt="image-20240606225546330" style="zoom:50%;" />

ä»æœ€åå®ç°çš„ç»“æœæ¥çœ‹ï¼Œæˆ‘çš„ä»£ç å®ç°ä¸­ï¼ŒåŸºäº Dueling Networkçš„ DQNå¯èƒ½ç”±äºæ™®é€šçš„ DQN å’Œ åŸºäºè½¯æ›´æ–°çš„ DQNã€‚æˆ‘å®ç°çš„æ™®é€šçš„DQNï¼Œåœ¨åæœŸç»“æœä¸æ˜¯å¾ˆç¨³å®šï¼Œæˆ‘è¯•äº†å¾ˆå¤šç§åŠæ³•ï¼Œå°è¯•ä½¿ç”¨ä¸‰å±‚å…¨è¿æ¥å±‚çš„ç¥ç»ç½‘ç»œæ¨¡å‹ï¼Œä½†æ˜¯æµ‹è¯•åæœŸçš„100å±€çš„å¹³å‡rewardä¸æ˜¯å¾ˆé«˜ã€‚





## å››ã€å‚è€ƒèµ„æ–™

* https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html
* https://blog.gofynd.com/building-a-deep-q-network-in-pytorch-fa1086aa5435

